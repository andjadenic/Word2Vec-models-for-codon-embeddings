# Exploring Word2Vec Models for Capturing the Similarity of Codon Embeddings

This repository includes [abstract](https://github.com/andjadenic/Word2Vec-models-for-codon-embeddings/blob/main/Conference%20Abstract%20EXPLORING%20WORD2VEC%20MODELS%20FOR%20CAPTURING%20THE%20SIMILARITY%20OF%20CODON%20EMBEDDINGS.pdf), [presentation](https://github.com/andjadenic/Word2Vec-models-for-codon-embeddings/blob/main/Conference%20Presentation%20Exploring%20WORD2VEC%20models%20for%20Capturing%20the%20Similarity%20of%20Codon%20Embeddings.pdf) and Python code for [3rd Serbian International Conference on Applied Artificial Intelligence (SICAAI)](http://aai2024.kg.ac.rs/) held in Kragujevac, Serbia on May, 2024, where I participated and got awarded for the best student work.

## YouTube video of conference talk
Click on the image below to watch the conference talk on YouTube.
[![YouTube video of conference talk](https://img.youtube.com/vi/KH8MOoOkKrI/0.jpg)](https://www.youtube.com/watch?v=KH8MOoOkKrI)



https://github.com/user-attachments/assets/c7087aeb-a6a9-438b-baa7-a2fb4b9620fb





### Word2Vec Model
[Word2vec](https://arxiv.org/abs/1310.4546), is a neural network (NN) technique, that learns a vector representation of words (tokens), analyzing its context (surrounding tokens), and use some function between
vectors to encode the semantic relationship between words.

### Word2Vec in Bioinformatics
Inspired by its success in NLP many researchers tried to replicate its idea to biological sequence embedding [ [1](https://www.nature.com/articles/s41598-020-80670-x), [2](https://dergipark.org.tr/en/download/article-file/967738), [3](https://arxiv.org/abs/1701.06279) ]. 
However, compared to the NLP problems where it is clear what the tokens and their context are,
in bioinformatics that is not the case.

### Exploring Different Word2Vec Models
A lot of papers embedded k-mers, but there are multiple options for the choice of a context and the length of k-mers, as well as whether there should be an overlap between k-mers.
Therefore, we will empirically analyze the quality of 3-mer (codon) embeddings of the genome sequences of [V. Cholerae](https://github.com/andjadenic/Word2Vec-models-for-codon-embeddings/blob/main/Vibrio_cholerae_genome.txt) and [E. Coli](https://github.com/andjadenic/Word2Vec-models-for-codon-embeddings/blob/main/Escherichi_coli.fna).

### Datasets
* The dataset was generated by sliding a window across the DNA sequences of two bacteria ( [V. Cholerae](https://github.com/andjadenic/Word2Vec-models-for-codon-embeddings/blob/main/Vibrio_cholerae_genome.txt) and [E. Coli](https://github.com/andjadenic/Word2Vec-models-for-codon-embeddings/blob/main/Escherichi_coli.fna)) and using codons along with their left and right neighbors as input.
* We experimented with different hyperparameters: numbers of neighbors ($m \in {3,5,10,20,40}$ ) and whether the windows overlap or not.

### Evaluation
We evaluate the results by analyzing the vector representation of the codons.
* There are 64 codons which are grouped into 21 categories according to the amino acids they encode [ref](https://pubmed.ncbi.nlm.nih.gov/38137022/) as we can see in the codon table below.
![codon table](https://i0.wp.com/www.biotechreality.com/wp-content/uploads/2024/11/Codon_Chart.png?w=935&ssl=1)
* For each codon, we define recall as the ratio between the number of codons from the same group found among its top 6 closest neighbors, determined by **cosine vector distance**, and the total number of codons in the group.
* The table in the [abstract](https://github.com/andjadenic/Word2Vec-models-for-codon-embeddings/blob/main/Conference%20Abstract%20EXPLORING%20WORD2VEC%20MODELS%20FOR%20CAPTURING%20THE%20SIMILARITY%20OF%20CODON%20EMBEDDINGS.pdf) presents the average recall across codons.

### Results
Experimental results reveal that using overlapping windows and about five neighbors results in the highest similarity among vector embeddings for codons within the same group.
